## 实验报告
密码都是：0fe3c9:~#
### 1. LeNet中哪些结构或思想在ResNet中仍然存在？哪些已经不用？
1. LeNet全局平均池化，resnet也是
2. 卷积层
3. 全连接层
4. 激活函数resnet使用RELU，Lenet使用的是sigmoid
### 2.	AlexNet对于LeNet做了哪些改进？
1. 使用层叠池化
   - 使用最大池化层 max pooling
   - 步长比池化的核的尺寸小，这样池化层的输出之间有重叠，提升了特征的丰富性
2. 使用数据增强
    - 镜像反射和随机剪裁
    - 改变训练样本RGB通道的强度值
3. 使用Dropout
   - Dropout操作会将概率小于0.5的每个隐层神经元的输出设为0，即去掉了一些神经节点，达到防止过拟合。
4. 使用ReLU激活函数
   - 加快收敛，防止过拟合
5. 使用LRN
   局部响应归一化（LRN）对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。
6. 网络层数增加
7. 多GPU并行训练

### 3.	这些改进中，有哪些在ResNet中仍然存在？哪些又舍弃了？
- 被舍弃
    1. 层叠池化
    2. dropout。使用
    3. LRN
- 仍然使用
    1. Relu激活
    2. 数据增强
    3. 网络层数加深
   
### 4.	如果再把舍弃的改进加回ResNet，会有什么样的实验表现？请挑选一处，在Tiny-ImageNet数据集上做实验，给出量化分析（可以从性能和运算效率角度出发；设置对照实验；如resnet现有结构不足以支撑你的实验，可改动其网络结构）。


## 代码说明